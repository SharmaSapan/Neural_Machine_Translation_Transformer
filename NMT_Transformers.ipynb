{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuRrReRV8Hjl"
      },
      "source": [
        "#**Neural Machine Translation using Transformers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfMV1hjblDLj"
      },
      "source": [
        "# nightly version of tensorflow was used as they have more features available\n",
        "!pip install -q tf-nightly > /dev/null 2>&1\n",
        "!pip install -q tensorflow_text_nightly > /dev/null 2>&1\n",
        "!pip install -q tensorflow_datasets\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "# from nightly\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1JAztSplITa"
      },
      "source": [
        "# to download the data-set, do not re-run if previously run\n",
        "# dataset is already in lowercase and have space before and after punctuation.\n",
        "tfds.disable_progress_bar()\n",
        "builder = tfds.builder('ted_hrlr_translate/pt_to_en', data_dir=os.getcwd())\n",
        "builder.download_and_prepare()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32NCKxI2lK1T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ea3cb517-2c61-4642-e847-f57794187170"
      },
      "source": [
        "# loads the dataset\n",
        "sets, info = tfds.load('ted_hrlr_translate/pt_to_en', data_dir=os.getcwd(),with_info=True, as_supervised=True, download=False)\n",
        "train, val, test = sets['train'], sets['validation'], sets['test']  # 51785, 1193, 1803 examples in respective sets\n",
        "for por_examples, eng_examples in test.batch(3).take(1):\n",
        "  for pt in por_examples.numpy():\n",
        "    print(\"Sample Portuguese: \", pt.decode('utf-8'))\n",
        "  for en in eng_examples.numpy():\n",
        "    print(\"Sample English: \", en.decode('utf-8'))\n",
        "\n",
        "# Bert pre-trained language representation is used to feed vectors to the transformer model. It enhances the language understanding,\n",
        "# by retaining context unlike word2vec or GloVe representation where each word only has one vector without context. \n",
        "# Bert gives bidirectional context using encoder based on other words in the sequence giving more exploitation potential to the model.\n",
        "\n",
        "# Bert wordpiece vocabulary of tensorflow is used, it uses Bert's splitting algorithm to split the text into words \n",
        "# before generating the subword vocabulary on the pre-trained bert by Tensorflow team. Example vocabulary for hello: he###, hell##, #ello, etc.\n",
        "reserved_tokens = [\"[START]\",\"[END]\",\"[PAD]\",\"[UNK]\",]\n",
        "bert_vocab_args = dict(\n",
        "    vocab_size = 5000,\n",
        "    bert_tokenizer_params=dict(lower_case=True),\n",
        "    reserved_tokens = reserved_tokens,\n",
        "    learn_params = {} \n",
        ")\n",
        "\n",
        "eng_train = train.map(lambda pt, en:en)\n",
        "por_train = train.map(lambda pt, en:pt)\n",
        "\n",
        "# creating the vocabulary file\n",
        "eng_vocab = bert_vocab.bert_vocab_from_dataset(eng_train.batch(1000).prefetch(2), **bert_vocab_args)\n",
        "with open(os.path.join(os.getcwd(),'eng_vocab.txt'), 'w') as f:\n",
        "  for tok in eng_vocab:\n",
        "    print(tok, file=f)\n",
        "por_vocab = bert_vocab.bert_vocab_from_dataset(por_train.batch(1000).prefetch(2), **bert_vocab_args)\n",
        "with open(os.path.join(os.getcwd(),'por_vocab.txt'), 'w') as f:\n",
        "  for tok in por_vocab:\n",
        "    print(tok,file=f)\n",
        "\n",
        "addons = [\"[START]\",\"[END]\",\"[PAD]\"]\n",
        "\n",
        "def add_reserved_tokens(vector):\n",
        "  count = vector.bounding_shape()[0]\n",
        "  starts = tf.fill([count,1], tf.argmax(tf.constant(addons) == \"[START]\"))\n",
        "  ends = tf.fill([count,1], tf.argmax(tf.constant(addons) == \"[END]\"))\n",
        "  return tf.concat([starts, vector, ends], axis=1)\n",
        "\n",
        "\n",
        "def post_process(words):\n",
        "  remove_addons = \"|\".join([re.escape(token) for token in addons])\n",
        "  result = tf.ragged.boolean_mask(words, ~tf.strings.regex_full_match(words, remove_addons))\n",
        "  return tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "\n",
        "\n",
        "# The bert wordpiece vocabulary is used by BertTokenizer to convert text string to wordpiece tokenization.\n",
        "class custom_Bert(tf.Module):\n",
        "  def __init__(self, vocab_path):\n",
        "    self.bert = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "    self.vocab_path = vocab_path\n",
        "    self.vocab = tf.Variable(pathlib.Path(vocab_path).read_text().splitlines())\n",
        "\n",
        "  # vectorize the given string to token ids, preprocess data, add start and end tokens\n",
        "  def tokenize(self, strings):\n",
        "    tokenized_string = self.bert.tokenize(strings).merge_dims(-2,-1)\n",
        "    return add_reserved_tokens(tokenized_string)\n",
        "\n",
        "  # recreates the sentence using encoded tokens\n",
        "  def detokenize(self, tokens):\n",
        "    return post_process(self.bert.detokenize(tokens))\n",
        "\n",
        "  # find the word from vocabulary using ids\n",
        "  def ids_to_word(self, ids):\n",
        "    return tf.gather(self.vocab, ids)\n",
        "  \n",
        "  def vocab_size(self):\n",
        "    return tf.shape(self.vocab)[0]\n",
        "\n",
        "\n",
        "bert_tokenizer = tf.Module()\n",
        "bert_tokenizer.eng = custom_Bert(os.path.join(os.getcwd(),'eng_vocab.txt'))\n",
        "bert_tokenizer.por = custom_Bert(os.path.join(os.getcwd(),'por_vocab.txt'))\n",
        "\n",
        "# positional encoding layer using sin and cosines on alternate positions to get positional context.\n",
        "def positional_encodings(position, embed_d):\n",
        "  frequencies = np.arange(position).reshape(position,1)/(np.power(10000,(2*(np.arange(embed_d).reshape(1,embed_d)//2)/embed_d)))\n",
        "  frequencies[:, 0::2] = np.sin(frequencies[:,0::2])\n",
        "  frequencies[:, 1::2] = np.cos(frequencies[:,1::2])\n",
        "  pos_enc = tf.cast(frequencies[np.newaxis, ...],tf.float32)\n",
        "  return pos_enc\n",
        "\n",
        "\n",
        "def feed_forward_network(embed_d, inner_d):\n",
        "  ffn = tf.keras.Sequential(\n",
        "      [\n",
        "       tf.keras.layers.Dense(inner_d, activation='relu'),\n",
        "       tf.keras.layers.Dense(embed_d)\n",
        "      ]\n",
        "  )\n",
        "  return ffn\n",
        "\n",
        "# attention to mask other values and only show seq which is being attended upon\n",
        "def dot_product_attention(query, key, value, decode_mask):\n",
        "  q_dot_k = tf.matmul(query,key,transpose_b=True)\n",
        "  d = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  scaled_product = q_dot_k/tf.math.sqrt(d)\n",
        "  if decode_mask is not None:\n",
        "    scaled_product += (decode_mask * -1e9)\n",
        "  attention_weights = tf.nn.softmax(scaled_product, axis=-1)\n",
        "  scaled_attention = tf.matmul(attention_weights, value)\n",
        "  return attention_weights, scaled_attention\n",
        "\n",
        "# used to split heads and calculate attention which allows the model to jointly attend\n",
        "# to the information from different representational dimensions.\n",
        "class MultiHeadedAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, embed_d, n_heads):\n",
        "    super(MultiHeadedAttention, self).__init__()\n",
        "    self.embed_d = embed_d\n",
        "    self.n_heads = n_heads\n",
        "    self.wq = tf.keras.layers.Dense(embed_d)\n",
        "    self.wk = tf.keras.layers.Dense(embed_d)\n",
        "    self.wv = tf.keras.layers.Dense(embed_d)\n",
        "    self.depth = embed_d//self.n_heads\n",
        "    self.dense = tf.keras.layers.Dense(embed_d)\n",
        "\n",
        "  def spliting_heads(self, qkv, batch_size):\n",
        "    qkv = tf.reshape(qkv, (batch_size, -1, self.n_heads, self.depth))\n",
        "    return tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    q = self.spliting_heads(self.wq(q), batch_size)\n",
        "    k = self.spliting_heads(self.wk(k), batch_size)\n",
        "    v = self.spliting_heads(self.wv(v), batch_size)\n",
        "\n",
        "    attention_weights, scaled_attention = dot_product_attention(q, k, v, mask)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])\n",
        "    concated_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embed_d))\n",
        "    output = self.dense(concated_attention)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Encoder architecture allows the self-attention where all of the keys, values and queries are same.\n",
        "# hence attenting to all positions in the previous step of layer.\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embed_d, n_heads, inner_d, drop_rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.multi_heads = MultiHeadedAttention(embed_d, n_heads)\n",
        "    self.dropout_layer1 = tf.keras.layers.Dropout(drop_rate)\n",
        "    self.layer_norm1 = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-6)\n",
        "    self.ffn = feed_forward_network(embed_d, inner_d)\n",
        "    self.dropout_layer2 = tf.keras.layers.Dropout(drop_rate)\n",
        "    self.layer_norm2 = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-6)\n",
        "\n",
        "  def call(self, v, mask, training):\n",
        "    encoder_self_attention, encoder_attention_weights = self.multi_heads(v,v,v, mask)\n",
        "    encoder_self_attention = self.dropout_layer1(encoder_self_attention, training=training)\n",
        "    normalized1 = self.layer_norm1(v+encoder_self_attention)\n",
        "    linear_transform = self.ffn(normalized1)\n",
        "    linear_transform = self.dropout_layer2(linear_transform, training=training)\n",
        "    normalized2 = self.layer_norm2(normalized1+linear_transform)\n",
        "    return normalized2\n",
        "\n",
        "# this module includes embedding layers for input, positional encodings and output layer to feed into decoder\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_enc_layers, embed_d, n_heads, inner_d, vocab_size, max_pos, drop_rate):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embed_d = embed_d\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_d)\n",
        "    self.pos_encoding = positional_encodings(max_pos, self.embed_d)\n",
        "    self.n_enc_layers = n_enc_layers\n",
        "    self.enc_layers = [EncoderLayer(embed_d, n_heads, inner_d, drop_rate) for layer in range(n_enc_layers)]\n",
        "    self.dropout_layer = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, v, mask, training):\n",
        "    input_seq_len = tf.shape(v)[1]\n",
        "    v = self.embedding_layer(v)\n",
        "    v *= tf.math.sqrt(tf.cast(self.embed_d, tf.float32))\n",
        "    v += self.pos_encoding[:, :input_seq_len, :]\n",
        "    v = self.dropout_layer(v, training=training)\n",
        "    for layer in range(self.n_enc_layers):\n",
        "      v = self.enc_layers[layer](v, mask, training)\n",
        "    return v\n",
        "\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embed_d, n_heads, inner_d, drop_rate):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.masked_multi_heads = MultiHeadedAttention(embed_d, n_heads)\n",
        "    self.dropout_layer1 = tf.keras.layers.Dropout(drop_rate)\n",
        "    self.layer_norm1 = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-6)\n",
        "    self.multi_heads = MultiHeadedAttention(embed_d, n_heads)\n",
        "    self.dropout_layer2 = tf.keras.layers.Dropout(drop_rate)\n",
        "    self.layer_norm2 = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-6)\n",
        "    self.ffn = feed_forward_network(embed_d, inner_d)\n",
        "    self.dropout_layer3 = tf.keras.layers.Dropout(drop_rate)\n",
        "    self.layer_norm3 = tf.keras.layers.LayerNormalization(axis=-1, epsilon=1e-6)\n",
        "\n",
        "  def call(self, v, look_ahead_mask, padding, training, encoder_output):\n",
        "    decoder_self_attention, decoder_self_attn_wts = self.masked_multi_heads(v, v, v, look_ahead_mask)\n",
        "    decoder_self_attention = self.dropout_layer1(decoder_self_attention, training=training)\n",
        "    normalized1 = self.layer_norm1(decoder_self_attention + v)\n",
        "    dec_enc_attention, dec_enc_attn_wts = self.multi_heads(encoder_output, encoder_output, normalized1, padding)\n",
        "    dec_enc_attention = self.dropout_layer2(dec_enc_attention, training=training)\n",
        "    normalized2 = self.layer_norm2(dec_enc_attention + normalized1)\n",
        "    linear_transform = self.ffn(normalized2)\n",
        "    linear_transform = self.dropout_layer3(linear_transform, training=training)\n",
        "    normalized3 = self.layer_norm3(linear_transform + normalized2)\n",
        "    return normalized3, decoder_self_attn_wts, dec_enc_attn_wts\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_dec_layers, embed_d, n_heads, inner_d, vocab_size, max_pos, drop_rate):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embed_d = embed_d\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_d)\n",
        "    self.pos_encoding = positional_encodings(max_pos, self.embed_d)\n",
        "    self.n_dec_layers = n_dec_layers\n",
        "    self.dec_layers = [DecoderLayer(embed_d, n_heads, inner_d, drop_rate) for layer in range(n_dec_layers)]\n",
        "    self.dropout_layer = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, v, look_ahead_mask, padding, training, encoder_output):\n",
        "    target_seq_len = tf.shape(v)[1]\n",
        "    v = self.embedding_layer(v)\n",
        "    v *= tf.math.sqrt(tf.cast(self.embed_d, tf.float32))\n",
        "    v += self.pos_encoding[:, :target_seq_len, :]\n",
        "    v = self.dropout_layer(v, training=training)\n",
        "    for layer in range(self.n_dec_layers):\n",
        "      v, decoder_self_attn_wts, dec_enc_attn_wts = self.dec_layers[layer](v, look_ahead_mask, padding, training, encoder_output)\n",
        "    return v, decoder_self_attn_wts, dec_enc_attn_wts\n",
        "\n",
        "\n",
        "class NMT_Transformer(tf.keras.Model):\n",
        "  def __init__(self, n_layers, embed_d, n_heads, inner_d, inp_vocab_size, tar_vocab_size, max_pos, drop_rate):\n",
        "    super(NMT_Transformer, self).__init__()\n",
        "    self.encoder = Encoder(n_layers, embed_d, n_heads, inner_d, inp_vocab_size, max_pos, drop_rate)\n",
        "    self.decoder = Decoder(n_layers, embed_d, n_heads, inner_d, tar_vocab_size, max_pos, drop_rate)\n",
        "    self.linear = tf.keras.layers.Dense(tar_vocab_size)\n",
        "  \n",
        "  def call(self, enc_dec_mask, look_ahead_mask, input, target, training):\n",
        "    encoder_output = self.encoder(input, enc_dec_mask, training)\n",
        "    decoder_output, decoder_self_attn_wts, dec_enc_attn_wts = self.decoder(target, look_ahead_mask, enc_dec_mask, training, encoder_output)\n",
        "    linear_output = self.linear(decoder_output)\n",
        "    return linear_output, decoder_self_attn_wts, dec_enc_attn_wts\n",
        "\n",
        "\n",
        "class AdaptiveLR(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, embed_d, warmup_steps):\n",
        "    super(AdaptiveLR, self).__init__()\n",
        "    self.embed_d = tf.cast(embed_d, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, schedule):\n",
        "    variable_lr = tf.math.rsqrt(self.embed_d)*tf.math.minimum(tf.math.rsqrt(schedule), schedule*(self.warmup_steps**-1.5))\n",
        "    return variable_lr\n",
        "\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "\n",
        "def loss_func(predicted, true):\n",
        "  sparse_cat_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = sparse_cat_loss(true, predicted)\n",
        "  pad_mask = tf.cast(tf.math.logical_not(tf.math.equal(true,0)), dtype=loss.dtype)\n",
        "  return tf.reduce_sum(loss*pad_mask)/tf.reduce_sum(pad_mask)\n",
        "\n",
        "\n",
        "def acc_func(predicted, true):\n",
        "  acc = tf.equal(true, tf.argmax(predicted, axis=2))\n",
        "  pad_mask = tf.math.logical_not(tf.math.equal(true,0))\n",
        "  acc = tf.math.logical_and(pad_mask, acc)\n",
        "  return tf.reduce_sum(tf.cast(acc, dtype=tf.float32))/tf.reduce_sum(tf.cast(pad_mask, dtype=tf.float32))\n",
        "\n",
        "\n",
        "def masks(input, target):\n",
        "  # padding masks will create a mask of 1's wherever there are 0's\n",
        "  enc_dec_values = tf.cast(tf.math.equal(input, 0), tf.float32) \n",
        "  enc_dec_mask = enc_dec_values[:, tf.newaxis, tf.newaxis, :]\n",
        "  # look ahead mask is used to hide the future tokens from an index in decoder as that index needs to be predicted \n",
        "  dec_look_ahead_mask = 1-tf.linalg.band_part(tf.ones((tf.shape(target)[1],tf.shape(target)[1])), -1, 0)\n",
        "  dec_pad_values = tf.cast(tf.math.equal(target, 0), tf.float32) \n",
        "  dec_target_mask = dec_pad_values[:, tf.newaxis, tf.newaxis, :]\n",
        "  dec_masked = tf.maximum(dec_look_ahead_mask, dec_target_mask)\n",
        "  return enc_dec_mask, dec_masked\n",
        "\n",
        "\n",
        "def bert_mapping(pt, en):\n",
        "  pt = bert_tokenizer.por.tokenize(pt).to_tensor()\n",
        "  en = bert_tokenizer.eng.tokenize(en).to_tensor()\n",
        "  return pt, en\n",
        "\n",
        "train_batch = train.cache().shuffle(10000).batch(64).map(bert_mapping, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "val_batch = val.cache().shuffle(10000).batch(64).map(bert_mapping, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "test_batch = test.cache().shuffle(10000).batch(64).map(bert_mapping, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "nmt = NMT_Transformer(n_layers=2, embed_d=64, n_heads=2, inner_d=256, inp_vocab_size=bert_tokenizer.eng.vocab_size(), tar_vocab_size=bert_tokenizer.por.vocab_size(), max_pos=500, drop_rate=0.1)\n",
        "lr = AdaptiveLR(embed_d=64, warmup_steps=1000)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "train_signature = [tf.TensorSpec(shape=(None,None), dtype=tf.int64), tf.TensorSpec(shape=(None,None), dtype=tf.int64)]\n",
        "\n",
        "@tf.function(input_signature=train_signature)\n",
        "def training(input, target):\n",
        "  # target input leaves last word in seq to predict\n",
        "  target_input = target[:,:-1]\n",
        "  # target input leaves first word in seq\n",
        "  target_real = target[:,1:]\n",
        "  enc_dec_mask, dec_masked = masks(input, target_input)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction, decoder_self_attn_wts, dec_enc_attn_wts = nmt(enc_dec_mask, dec_masked, input, target_input, training=True)\n",
        "    loss = loss_func(prediction, target_real)\n",
        "  \n",
        "  grad = tape.gradient(loss, nmt.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grad, nmt.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(acc_func(prediction,target_real))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Portuguese:  depois , podem fazer-se e testar-se previsões .\n",
            "Sample Portuguese:  forçou a parar múltiplos laboratórios que ofereciam testes brca .\n",
            "Sample Portuguese:  as formigas são um exemplo clássico ; as operárias trabalham para as rainhas e vice-versa .\n",
            "Sample English:  then , predictions can be made and tested .\n",
            "Sample English:  it had forced multiple labs that were offering brca testing to stop .\n",
            "Sample English:  ants are a classic example ; workers work for queens and queens work for workers .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFSH9qfHmC6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d8e2c5de-0939-4631-900d-aa133c0b5320"
      },
      "source": [
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  for (batch, (por, eng)) in enumerate(train_batch):\n",
        "    training(eng,por)\n",
        "    if batch % 100 ==0:\n",
        "      print(f'Epoch no. {epoch+1} Batch {batch} Loss {train_loss.result():.3f} Accuracy {train_accuracy.result():.3f}')\n",
        "  print(f'Epoch no. {epoch + 1} Loss {train_loss.result():.3f} Accuracy {train_accuracy.result():.3f}')\n",
        "nmt.save_weights(os.path.join(os.getcwd(), 'weights'))\n",
        "\n",
        "def translate(sentence, max_seq_len=40):\n",
        "  tokenized_input = bert_tokenizer.eng.tokenize(tf.convert_to_tensor([sentence])).to_tensor()\n",
        "  start_token, end_token = bert_tokenizer.por.tokenize([''])[0]\n",
        "  target = tf.expand_dims(tf.convert_to_tensor([start_token]),0)\n",
        "  for token in range(max_seq_len):\n",
        "    enc_dec_mask, dec_masked = masks(tokenized_input, target)\n",
        "    prediction, decoder_self_attn_wts, dec_enc_attn_wts = nmt(enc_dec_mask, dec_masked, tokenized_input, target, training=False)\n",
        "    prediction_id = tf.argmax(prediction[:, -1:, :], axis=-1)\n",
        "    target = tf.concat([target, prediction_id], axis=-1)\n",
        "    if prediction_id == end_token:\n",
        "      break\n",
        "\n",
        "  translated_text = bert_tokenizer.por.detokenize(target)[0]\n",
        "  translated_tokens = bert_tokenizer.por.ids_to_word(target)[0]\n",
        "  return translated_text, translated_tokens, decoder_self_attn_wts, dec_enc_attn_wts"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch no. 1 Batch 0 Loss 8.506 Accuracy 0.000\n",
            "Epoch no. 1 Batch 100 Loss 8.093 Accuracy 0.028\n",
            "Epoch no. 1 Batch 200 Loss 7.349 Accuracy 0.044\n",
            "Epoch no. 1 Batch 300 Loss 6.865 Accuracy 0.075\n",
            "Epoch no. 1 Batch 400 Loss 6.507 Accuracy 0.100\n",
            "Epoch no. 1 Batch 500 Loss 6.227 Accuracy 0.120\n",
            "Epoch no. 1 Batch 600 Loss 6.000 Accuracy 0.137\n",
            "Epoch no. 1 Batch 700 Loss 5.807 Accuracy 0.152\n",
            "Epoch no. 1 Batch 800 Loss 5.639 Accuracy 0.165\n",
            "Epoch no. 1 Loss 5.626 Accuracy 0.166\n",
            "Epoch no. 2 Batch 0 Loss 4.378 Accuracy 0.267\n",
            "Epoch no. 2 Batch 100 Loss 4.334 Accuracy 0.269\n",
            "Epoch no. 2 Batch 200 Loss 4.276 Accuracy 0.275\n",
            "Epoch no. 2 Batch 300 Loss 4.237 Accuracy 0.278\n",
            "Epoch no. 2 Batch 400 Loss 4.190 Accuracy 0.283\n",
            "Epoch no. 2 Batch 500 Loss 4.141 Accuracy 0.288\n",
            "Epoch no. 2 Batch 600 Loss 4.095 Accuracy 0.293\n",
            "Epoch no. 2 Batch 700 Loss 4.047 Accuracy 0.298\n",
            "Epoch no. 2 Batch 800 Loss 4.003 Accuracy 0.303\n",
            "Epoch no. 2 Loss 3.999 Accuracy 0.304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko2qmyD-mN6J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "99c5b1d3-5dc0-4664-972e-274d9cbcdced"
      },
      "source": [
        "def test_bleu():\n",
        "  test_input = []\n",
        "  test_predicted = []\n",
        "  test_truth = []\n",
        "  bleu_scores = []\n",
        "  for por_examples, eng_examples in test.batch(30).take(10):\n",
        "    for pt in por_examples.numpy():\n",
        "      test_truth.append(pt.decode('utf-8'))\n",
        "    for en in eng_examples.numpy():\n",
        "      test_input.append(en.decode('utf-8'))\n",
        "      translated_text, _, _, _ = translate(en.decode('utf-8'))\n",
        "      test_predicted.append(translated_text.numpy().decode(\"utf-8\"))\n",
        "\n",
        "  bleu1 = corpus_bleu(test_truth, test_predicted, weights=(1.0, 0, 0, 0))\n",
        "  print('BLEU-1 results: {:.3f}'.format(bleu1))\n",
        "  bleu2 = corpus_bleu(test_truth, test_predicted, weights=(0.5, 0.5, 0, 0))\n",
        "  print('BLEU-2 results: {:.3f}'.format(bleu2))\n",
        "  bleu3 = corpus_bleu(test_truth, test_predicted, weights=(0.3, 0.3, 0.3, 0))\n",
        "  print('BLEU-3 results: {:.3f}'.format(bleu3))\n",
        "  bleu4 = corpus_bleu(test_truth, test_predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  print('BLEU-4 results: {:.3f}'.format(bleu4))\n",
        "  bleu_scores = [bleu1, bleu2, bleu3, bleu4]\n",
        "  return test_input, test_predicted, test_truth, bleu_scores\n",
        "\n",
        "print(\"Bleu scores computed on 300 examples from test set and sample results: \")\n",
        "test_input, test_predicted, test_truth, bleu_scores = test_bleu()\n",
        "for ind in range(6):\n",
        "  print(\"Sample Input: \", test_input[ind])\n",
        "  print(\"Sample Predicted: \", test_predicted[ind])\n",
        "  print(\"Sample Truth: \", test_truth[ind])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu scores computed on 300 examples from test set and sample results: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-1 results: 0.143\n",
            "BLEU-2 results: 0.378\n",
            "BLEU-3 results: 0.557\n",
            "BLEU-4 results: 0.615\n",
            "Sample Input:  then , predictions can be made and tested .\n",
            "Sample Predicted:  ` ` ` ` ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' , exproventaveis . '\n",
            "Sample Truth:  depois , podem fazer-se e testar-se previsões .\n",
            "Sample Input:  it had forced multiple labs that were offering brca testing to stop .\n",
            "Sample Predicted:  ` ` ` ` ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
            "Sample Truth:  forçou a parar múltiplos laboratórios que ofereciam testes brca .\n",
            "Sample Input:  ants are a classic example ; workers work for queens and queens work for workers .\n",
            "Sample Predicted:  ` ` ` ` ' ' ' ' um exemplo , os dias , os dias expeitos para os trabalhadores . ' '\n",
            "Sample Truth:  as formigas são um exemplo clássico ; as operárias trabalham para as rainhas e vice-versa .\n",
            "Sample Input:  one of every hundred children born worldwide has some kind of heart disease .\n",
            "Sample Predicted:  ` ` ` ` ' ' ' ' de todos os miudos do mundo . ' ' '\n",
            "Sample Truth:  uma em cada cem crianças no mundo nascem com uma doença cardíaca .\n",
            "Sample Input:  at this point in her life , she 's suffering with full-blown aids and had pneumonia .\n",
            "Sample Predicted:  ` ` ` ` ' ' ' o seu ponto de ` ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' . '\n",
            "Sample Truth:  neste momento da sua vida , ela está a sofrer de sida no seu expoente máximo e tinha pneumonia .\n",
            "Sample Input:  where are economic networks ?\n",
            "Sample Predicted:  ` ` ` ` ' ' ' onde sao as redes sociais ? ' ' '\n",
            "Sample Truth:  onde estão as redes económicas ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtn1EL8Embz-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6f384bdf-65bb-4ad7-da61-73e1e8d0a118"
      },
      "source": [
        "sample_text = \"where are we going after lunch, do you have any plans or we should make on our way?\"\n",
        "translated_text, _, _, _ = translate(sample_text)\n",
        "print(translated_text.numpy().decode(\"utf-8\"))\n",
        "\n",
        "# truth: as formigas são um exemplo clássico ; as operárias trabalham para as rainhas e vice-versa .\n",
        "sample_text3 = \"ants are a classic example ; workers work for queens and queens work for workers.\"\n",
        "translated_text, _, _, _ = translate(sample_text3)\n",
        "print(translated_text.numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "` ` ` ` ' ' ' onde nos vamos ter descordar ou os nossos estudos ? ' ' '\n",
            "` ` ` ` ' ' ' ' um exemplo , os dias , os dias expeitos para os trabalhadores . ' '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JgQ89Y08lax"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}